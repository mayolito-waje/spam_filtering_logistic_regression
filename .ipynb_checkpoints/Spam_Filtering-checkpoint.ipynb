{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df94860d-6ba9-408d-86bc-845d381840ed",
   "metadata": {},
   "source": [
    "# Spam Filtering using Logistic Regression\n",
    "\n",
    "Spam messages / emails are one of the common problems we encounter in our life. Numerous spam filtering methods are made which helped us to block those emails. In this project, we will delve on how does spam filtering work by making it from scratch using binary classification with logistic regression. \n",
    "\n",
    "Before proceeding, let's look at the worst spams in history:\n",
    "\n",
    "- **ARPANET Incident (1978)**: Gary Thuerk, spreaded the first SPAM in history targeting 393 ARPANET users, this caused an uproar and leads to creation of network etiquette guidelines.\n",
    "- **Canter & Siegel Incident (1994)**: This is where the first anti-spamming policies and guidelines was created. Law firm Canter & Siegel spammed Usenet newsgroups with ads for their services.\n",
    "- **AOL Incident (1995)**: A teenager with alias \"MafiaBoy\" spammed AOL users, this caused AOL to invest in spam filters.\n",
    "\n",
    "More examples [here](https://softwarelab.org/blog/spam-examples/)\n",
    "\n",
    "Now let's dive to implementation of SPAM filtering program through the use of *Supervised Machine Learning with Logistic Regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23223fd6-b1af-4b93-a250-dcc875285caa",
   "metadata": {},
   "source": [
    "## Importing the required libraries\n",
    "\n",
    "Let's import the required python libraries that we will utilize to create our programs. In addition, let's download the following corpus from the `NLTK` library:\n",
    "\n",
    "- `stopwords`: list of stopwords that will help us to filter unnecessary words that is not relevant for our analysis and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "394d64b7-8478-4621-be05-a8e40e8a195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mayo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mayo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28680ff-9a9a-44bc-9f98-0748dabe17e9",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "First, we need a dataset with labels (if they are spam or not). We will use those labels as our guide to train our data so whenever we see an unknown email who is not used for the training process, we can use the trained program to detect if it is a spam or not.\n",
    "\n",
    "Let's load the `emails.csv` from dataset folder and store it in a pandas `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d4db14-68f5-4d3c-be3d-7e11e0843a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subject: great nnews  hello , welcome to medzo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subject: here ' s a hot play in motion  homela...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subject: undeliverable : home based business f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1\n",
       "5  Subject: great nnews  hello , welcome to medzo...     1\n",
       "6  Subject: here ' s a hot play in motion  homela...     1\n",
       "7  Subject: save your money buy getting this thin...     1\n",
       "8  Subject: undeliverable : home based business f...     1\n",
       "9  Subject: save your money buy getting this thin...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/emails.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef91d4-79de-479a-a3d0-c2c65bccd36d",
   "metadata": {},
   "source": [
    "## Separate the spam and non-spam emails\n",
    "\n",
    "Let us separate the spam and non-spam emails so it can allow us to easily access them for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16132c36-efe5-4255-a3c9-5898619a56aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam emails length: 1368\n",
      "non-spam emails length: 4360\n",
      "\n",
      "spam emails examples:\n",
      "\tSubject: naturally irresistible your corporate identity  lt is really hard to recollect a company : the  market is full of suqgestions and the information isoverwhelminq ; but a good  catchy logo , st...\n",
      "\tSubject: the stock trading gunslinger  fanny is merrill but muzo not colza attainder and penultimate like esmark perspicuous ramble is segovia not group try slung kansas tanzania yes chameleon or cont...\n",
      "\tSubject: unbelievable new homes made easy  im wanting to show you this  homeowner  you have been pre - approved for a $ 454 , 169 home loan at a 3 . 72 fixed rate .  this offer is being extended to yo...\n",
      "\n",
      "non-spam emails examples:\n",
      "\tSubject: hello guys ,  i ' m \" bugging you \" for your completed questionnaire and for a one - page  bio / statement on your thoughts on \" business edu and the new economy \" . if  my records are incorr...\n",
      "\tSubject: sacramento weather station  fyi  - - - - - - - - - - - - - - - - - - - - - - forwarded by mike a roberts / hou / ect on 09 / 20 / 2000  09 : 06 am - - - - - - - - - - - - - - - - - - - - - - ...\n",
      "\tSubject: from the enron india newsdesk - jan 18 th newsclips  vince ,  fyi .  - - - - - - - - - - - - - - - - - - - - - - forwarded by sandeep kohli / enron _ development on  01 / 19 / 2001 05 : 12 pm...\n"
     ]
    }
   ],
   "source": [
    "spams = df[df['spam'] == 1]['text'].values\n",
    "non_spams = df[df['spam'] == 0]['text'].values\n",
    "\n",
    "print('spam emails length:', len(spams))\n",
    "print('non-spam emails length:', len(non_spams))\n",
    "\n",
    "print('\\nspam emails examples:')\n",
    "for i in range(3):\n",
    "    print(f'\\t{spams[i][:200]}...')\n",
    "\n",
    "print('\\nnon-spam emails examples:')\n",
    "for j in range(3):\n",
    "    print(f'\\t{non_spams[j][:200]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d71c54-d638-41b2-9d12-dacc06c0dba3",
   "metadata": {},
   "source": [
    "## Separate the training data from test data\n",
    "\n",
    "We will use the 80% of data based on their class to use for our training and the 20% or our testing. This will help us to check the accuracy of our model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5acc173-2658-4ba7-b23a-a6358fcdb186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length: 4582\n",
      "train_y length: 4582\n",
      "test_x length: 1146\n",
      "test_y length: 1146\n"
     ]
    }
   ],
   "source": [
    "spam_training_count = int(len(spams) * 0.8) # Use the 80% of the spam dataset for training\n",
    "non_spam_training_count = int(len(non_spams) * 0.8) # Use the 80% of the non-spam dataset for training\n",
    "\n",
    "spam_train = spams[:spam_training_count]\n",
    "spam_test = spams[spam_training_count:]\n",
    "non_spam_train = non_spams[:non_spam_training_count]\n",
    "non_spam_test = non_spams[non_spam_training_count:]\n",
    "\n",
    "train_x = np.append(spam_train, non_spam_train, axis=0)\n",
    "test_x = np.append(spam_test, non_spam_test, axis=0)\n",
    "train_y = np.append(np.ones((len(spam_train), 1)), np.zeros((len(non_spam_train), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(spam_test), 1)), np.zeros((len(non_spam_test), 1)), axis=0)\n",
    "\n",
    "print('train_x length:', len(train_x))\n",
    "print('train_y length:', len(train_y))\n",
    "print('test_x length:', len(test_x))\n",
    "print('test_y length:', len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50f242-a40c-488a-84d0-ad7e2921d9dd",
   "metadata": {},
   "source": [
    "## Natural Language Preprocessing\n",
    "\n",
    "First, we have to clean our text through the use of preprocessing before extracting features from our text. The steps in preprocessing of text for binary classification is the following:\n",
    "\n",
    "- removing links, mail addresses, etc.\n",
    "- tokenizing texts\n",
    "- lowercasing\n",
    "- removing punctuations\n",
    "- removing stopwords\n",
    "- stemming\n",
    "\n",
    "Now let's write a function `process_text` to preprocess our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efadc1fc-129e-436e-b58c-2e0b9deef14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def process_text(text):\n",
    "    '''Preprocess a text and return a token of stem of words that is relevant for feature extraction\n",
    "    Parameters:\n",
    "        - text: a string to preprocess\n",
    "    '''\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    \n",
    "    text = re.sub(r'^Subject:', '', text, flags=re.I) # Remove 'Subject:' at start\n",
    "    text = re.sub(r'\\b([0-9]*)\\b', '', text) # Remove numbers\n",
    "    \n",
    "    added_stopwords = ['re', 'cc', 'bcc', 'subject', 'hpl', 'hou', 'enron']\n",
    "    \n",
    "    text_stems = []\n",
    "    \n",
    "    text_tokens = tokenizer.tokenize(text)\n",
    "    for token in text_tokens:\n",
    "        if token not in string.punctuation and token not in eng_stopwords and token not in added_stopwords:\n",
    "            stem = stemmer.stem(token)\n",
    "            text_stems.append(stem)\n",
    "    \n",
    "    return text_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f45d6a-673c-42e0-95d5-37899f52bec1",
   "metadata": {},
   "source": [
    "### Testing `process_text`\n",
    "\n",
    "Let's test our function if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d058447-5c55-472e-ac5b-51c06d233da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-processed Text:\n",
      "\t-\n",
      "Subject: letter from : daniel kabila  letter from : daniel kabila  investment offer .  dear ,  in appreciation of your esteemed contact received through a reliable source  and the choice of your country i wish to introduce myself , i am daniel kabila  the son of the late drc president laurent desire kabila of the blessed memory .  i know this letter might come to you as a surprise but i honestly do not  intend to surprise you . i write this letter in respect of my intention to  invest the sum of us $ 12 m ( twelve million united state dollars ) with you . i  inherited this money from my mother . this money was got through the smuggling  and sales of diamond and timber when my father was the head of state . my  mother though not her legal wife used her privilege position to engage in  the  business of diamond and timber since she knows that her survival will depend  on how much she can get out of the privilege situation .  when my father was assassinated on 16 th jan . 01 by one of his bodyguards  lt . rashidi kasereke through the conspiracy of some top army officers that  wanted to topple him i escaped to sa because of the fear that i might be  arrested by my half brother lt . general joseph kabila the present head of  state . actually his mother and my mother are not in the best of relationship  because of who among them will be the first lady tussle and this ultimately  affected us their children . considering the relationship between sa and my  country ' s new government , my mother advised me to leave for sa for security  reason , while the funds were deposited with a security company abroad .  on getting to there where i have been living since then as a political refugee  i am seeking for a reliable foreigner who can assist me in moving this money  out for safe banking and profitable investment . honestly i contacted you  because i don ' t want to invest this money in here due to my status here as  a  political refugee . and moreover i wouldn ' t want to take risk because this  money is all that i and my mother is depending on because my half brother  has  seized all my father ' s assets and money and left i and my mother empty handed  without knowing about this funds deposited at the security company in abroad  so that is why i decided that investing this money abroad should be the best  investment for me . i will be honored if i can be given the privilege of  investing this money with your help .  in view of this plight , i expect you to be trustworthy and kind enough to  respond to this distress call to save my mother and i from a hopeless future .  and if you agree , i hereby agree to compensate your sincere and candid effort  in this regard with 15 % of the total money and annual 5 % of the after  tax returns on investment for the first three years . thereafter , the term  shall be varied . 5 % for expenses , which may arise during the transaction (  fax  and phone bills inclusive ) . when the money is moved into your discrete  account , you will be allowed to draw 15 % in your favor , while the remaining  80 % will be invested meaningfully for our future if possible in your area  of  business and deterrents sectors of the economy in your country which are  dividends yielding .  whatever your decision is please reach me immediately through my email , and  keep this letter tight secret for the interest of my family .  best regards ,  daniel kabila\n",
      "\n",
      "\n",
      "Processed Text:\n",
      "\t-\n",
      "['letter', 'daniel', 'kabila', 'letter', 'daniel', 'kabila', 'invest', 'offer', 'dear', 'appreci', 'esteem', 'contact', 'receiv', 'reliabl', 'sourc', 'choic', 'countri', 'wish', 'introduc', 'daniel', 'kabila', 'son', 'late', 'drc', 'presid', 'laurent', 'desir', 'kabila', 'bless', 'memori', 'know', 'letter', 'might', 'come', 'surpris', 'honestli', 'intend', 'surpris', 'write', 'letter', 'respect', 'intent', 'invest', 'sum', 'us', 'twelv', 'million', 'unit', 'state', 'dollar', 'inherit', 'money', 'mother', 'money', 'got', 'smuggl', 'sale', 'diamond', 'timber', 'father', 'head', 'state', 'mother', 'though', 'legal', 'wife', 'use', 'privileg', 'posit', 'engag', 'busi', 'diamond', 'timber', 'sinc', 'know', 'surviv', 'depend', 'much', 'get', 'privileg', 'situat', 'father', 'assassin', 'th', 'jan', 'one', 'bodyguard', 'lt', 'rashidi', 'kaserek', 'conspiraci', 'top', 'armi', 'offic', 'want', 'toppl', 'escap', 'sa', 'fear', 'might', 'arrest', 'half', 'brother', 'lt', 'gener', 'joseph', 'kabila', 'present', 'head', 'state', 'actual', 'mother', 'mother', 'best', 'relationship', 'among', 'first', 'ladi', 'tussl', 'ultim', 'affect', 'us', 'children', 'consid', 'relationship', 'sa', 'countri', 'new', 'govern', 'mother', 'advis', 'leav', 'sa', 'secur', 'reason', 'fund', 'deposit', 'secur', 'compani', 'abroad', 'get', 'live', 'sinc', 'polit', 'refuge', 'seek', 'reliabl', 'foreign', 'assist', 'move', 'money', 'safe', 'bank', 'profit', 'invest', 'honestli', 'contact', 'want', 'invest', 'money', 'due', 'statu', 'polit', 'refuge', 'moreov', 'want', 'take', 'risk', 'money', 'mother', 'depend', 'half', 'brother', 'seiz', 'father', 'asset', 'money', 'left', 'mother', 'empti', 'hand', 'without', 'know', 'fund', 'deposit', 'secur', 'compani', 'abroad', 'decid', 'invest', 'money', 'abroad', 'best', 'invest', 'honor', 'given', 'privileg', 'invest', 'money', 'help', 'view', 'plight', 'expect', 'trustworthi', 'kind', 'enough', 'respond', 'distress', 'call', 'save', 'mother', 'hopeless', 'futur', 'agre', 'herebi', 'agre', 'compens', 'sincer', 'candid', 'effort', 'regard', 'total', 'money', 'annual', 'tax', 'return', 'invest', 'first', 'three', 'year', 'thereaft', 'term', 'shall', 'vari', 'expens', 'may', 'aris', 'transact', 'fax', 'phone', 'bill', 'inclus', 'money', 'move', 'discret', 'account', 'allow', 'draw', 'favor', 'remain', 'invest', 'meaning', 'futur', 'possibl', 'area', 'busi', 'deterr', 'sector', 'economi', 'countri', 'dividend', 'yield', 'whatev', 'decis', 'pleas', 'reach', 'immedi', 'email', 'keep', 'letter', 'tight', 'secret', 'interest', 'famili', 'best', 'regard', 'daniel', 'kabila']\n"
     ]
    }
   ],
   "source": [
    "sample_mail = train_x[55]\n",
    "processed = process_text(sample_mail)\n",
    "\n",
    "print(f'Non-processed Text:\\n\\t-{sample_mail[:100]}...')\n",
    "print('\\n\\nProcessed Text:\\n\\t-')\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416cec4-4e2e-4d20-9abf-27e67d92e505",
   "metadata": {},
   "source": [
    "## Building frequencies\n",
    "\n",
    "Now, we have to build frequencies of how many times a words appeared based on their corresponding labels (spam or non-spam). For this, we will use the python dictionary to allow for faster searching. Let's build a function `build_frequencies` where it will accept a list of texts and list of labels as parameters. \n",
    "\n",
    "The function will return a dictionary with keys `(word, label)` and their corresponding counts based on how many times they appeared on the entire dataset based on their corresponding label as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0836d920-9cd4-4307-8fba-d4816ca67438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frequencies(texts, ys):\n",
    "    '''Return a dictionary with tuple of (word, label) as keys\n",
    "    and values are how many times a word appeared on the dataset\n",
    "    based on their corresponding label.\n",
    "    \n",
    "    Parameters:\n",
    "        - xlist: a list of texts\n",
    "        - ylist: a list of labels corresponding to texts\n",
    "    '''\n",
    "    textslist = np.squeeze(texts).tolist()\n",
    "    labels = np.squeeze(ys).tolist()\n",
    "    \n",
    "    freqs = {}\n",
    "    for label, text in zip(labels, textslist):\n",
    "        for word in process_text(text):\n",
    "            key = (word, label)\n",
    "            freqs[key] = freqs.get(key, 0) + 1\n",
    "    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b61c1-e1c7-4335-bafd-05553b40fc46",
   "metadata": {},
   "source": [
    "### Testing `build_frequencies`\n",
    "\n",
    "Let's test the function with just one item list to check if it is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab832ab-a5e8-4380-9a65-ee4d304f0516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('natur', 1): 1, ('irresist', 1): 1, ('corpor', 1): 1, ('ident', 1): 1, ('lt', 1): 1, ('realli', 1): 1, ('hard', 1): 1, ('recollect', 1): 1, ('compani', 1): 3, ('market', 1): 4, ('full', 1): 1, ('suqgest', 1): 1, ('inform', 1): 1, ('isoverwhelminq', 1): 1, ('good', 1): 2, ('catchi', 1): 1, ('logo', 1): 4, ('stylish', 1): 1, ('statloneri', 1): 1, ('outstand', 1): 1, ('websit', 1): 2, ('make', 1): 2, ('task', 1): 1, ('much', 1): 2, ('easier', 1): 1, ('promis', 1): 2, ('havinq', 1): 1, ('order', 1): 1, ('iogo', 1): 1, ('automaticaili', 1): 1, ('becom', 1): 2, ('world', 1): 1, ('ieader', 1): 1, ('isguit', 1): 1, ('ciear', 1): 1, ('without', 1): 1, ('product', 1): 1, ('effect', 1): 2, ('busi', 1): 2, ('organ', 1): 1, ('practic', 1): 1, ('aim', 1): 1, ('hotat', 1): 1, ('nowaday', 1): 1, ('effort', 1): 1, ('list', 1): 1, ('clear', 1): 1, ('benefit', 1): 1, ('creativ', 1): 1, ('hand', 1): 1, ('made', 1): 1, ('origin', 1): 1, ('special', 1): 1, ('done', 1): 1, ('reflect', 1): 1, ('distinct', 1): 1, ('imag', 1): 1, ('conveni', 1): 1, ('stationeri', 1): 1, ('provid', 1): 2, ('format', 1): 1, ('easi', 1): 1, ('use', 1): 1, ('content', 1): 2, ('manag', 1): 1, ('system', 1): 1, ('letsyou', 1): 1, ('chang', 1): 2, ('even', 1): 1, ('structur', 1): 1, ('prompt', 1): 1, ('see', 1): 1, ('draft', 1): 1, ('within', 1): 1, ('three', 1): 1, ('day', 1): 1, ('afford', 1): 1, ('break', 1): 1, ('gap', 1): 1, ('budget', 1): 1, ('satisfact', 1): 1, ('guarante', 1): 1, ('unlimit', 1): 1, ('amount', 1): 1, ('extra', 1): 1, ('fee', 1): 1, ('surethat', 1): 1, ('love', 1): 1, ('result', 1): 1, ('collabor', 1): 1, ('look', 1): 1, ('portfolio', 1): 1, ('interest', 1): 1, ('. . .', 1): 1, ('hello', 0): 1, ('guy', 0): 1, ('bug', 0): 1, ('complet', 0): 1, ('questionnair', 0): 3, ('one', 0): 1, ('page', 0): 2, ('bio', 0): 2, ('statement', 0): 1, ('thought', 0): 1, ('busi', 0): 1, ('edu', 0): 3, ('new', 0): 1, ('economi', 0): 1, ('record', 0): 1, ('incorrect', 0): 1, ('pleas', 0): 1, ('ship', 0): 2, ('respons', 0): 1, ('want', 0): 1, ('put', 0): 1, ('everyth', 0): 1, ('togeth', 0): 1, ('next', 0): 1, ('week', 0): 2, ('back', 0): 1, ('everyon', 0): 1, ('attach', 0): 1, ('well', 0): 1, ('copi', 0): 2, ('michael', 0): 1, ('froehl', 0): 1, ('two', 0): 2, ('somewhat', 0): 1, ('differ', 0): 1, ('approach', 0): 2, ('idea', 0): 1, ('latter', 0): 1, ('introduc', 0): 1, ('panelist', 0): 1, ('give', 0): 1, ('background', 0): 2, ('issu', 0): 1, ('discuss', 0): 2, ('also', 0): 1, ('provid', 0): 1, ('attende', 0): 1, ('use', 0): 1, ('materi', 0): 1, ('person', 0): 1, ('introduct', 0): 1, ('open', 0): 1, ('panel', 0): 1, ('thank', 0): 1, ('look', 0): 1, ('forward', 0): 1, ('see', 0): 1, ('john', 0): 2, ('waco', 0): 2, ('mf', 0): 1, ('doc', 0): 3, ('jmartinbiosketch', 0): 1, ('martin', 0): 2, ('carr', 0): 1, ('p', 0): 1, ('collin', 0): 1, ('chair', 0): 1, ('financ', 0): 2, ('depart', 0): 1, ('baylor', 0): 3, ('univers', 0): 1, ('po', 0): 1, ('box', 0): 1, ('tx', 0): 1, ('offic', 0): 1, ('fax', 0): 1, ('j', 0): 1, ('web', 0): 1, ('http', 0): 1, ('hsb', 0): 1, ('html', 0): 2, ('martinj', 0): 1, ('home', 0): 1}\n"
     ]
    }
   ],
   "source": [
    "test_freq_x = [spam_train[0], non_spam_train[0]]\n",
    "test_freq_y = [1, 0]\n",
    "\n",
    "print(build_frequencies(test_freq_x, test_freq_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebfe33-eff1-405a-ae50-66feaee01809",
   "metadata": {},
   "source": [
    "It works! Now let's do this for the whole training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d49f81-9295-4def-96be-71c94b345b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of frequencies dictionary: 27923\n"
     ]
    }
   ],
   "source": [
    "freqs = build_frequencies(train_x, train_y)\n",
    "\n",
    "print('Total length of frequencies dictionary:', len(freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d5744-31b0-4808-8876-9e0577813f9b",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "\n",
    "Now, let's extract the features of the mail by writing function `extract_features`. The function will return a list with 3 elements: \n",
    "\n",
    "- the first element is the bias (which has value 1)\n",
    "- the second element is the number of times where the words of the text appear in frequencies dictionary with spam labels\n",
    "- the last element is the number of times where the words of the text appear in frequencies dictionary with non-spam labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f1858a0-c56d-4ab8-a565-9c40d1301779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, freqs):\n",
    "    '''Extract features from preprocessed text.\n",
    "    Parameters:\n",
    "        - text: a string, particularly a mail\n",
    "        - freqs: a dictionary with a using key (word, label) which contains \n",
    "        counts or how many times a word appeard based on label\n",
    "    '''\n",
    "    tokens = process_text(text)\n",
    "    \n",
    "    features = np.zeros(3)\n",
    "    features[0] = 1\n",
    "    \n",
    "    for word in tokens:\n",
    "        features[1] += freqs.get((word, 1.0), 0)\n",
    "        features[2] += freqs.get((word, 0), 0)\n",
    "        \n",
    "    features = features[None, :] # add batch dimension for further processing\n",
    "    assert(features.shape == (1, 3))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57cde4-a057-4f1f-9dbf-13850c735f57",
   "metadata": {},
   "source": [
    "### Testing `extract_features`\n",
    "\n",
    "Now let's test the function with the previous sample mail we tested to check that it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f1dfa56-55c2-4ec6-92f0-74b22e0a48ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mail:\n",
      "\t-Subject: letter from : daniel kabila  letter from : daniel kabila  investment offer .  dear ,  in ap...\n",
      "Feature: [[1.0000e+00 3.5115e+04 8.0128e+04]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Sample Mail:\\n\\t-{sample_mail[:100]}...')\n",
    "print('Feature:', extract_features(sample_mail, freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94e1b5-d5ee-4a8f-8f0a-e9dab1f58892",
   "metadata": {},
   "source": [
    "## Collect features and visualize\n",
    "\n",
    "Let's collect the features of our data and visualize them with scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204beea-6a12-4113-af52-00f576e688bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :] = extract_features(train_x[i], freqs)\n",
    "\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a460320-171d-4d70-bdc7-e16f8fe2a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "colors = ['green', 'red']\n",
    "\n",
    "ax.scatter(X[:, 2], X[:, 1], c=[colors[int(k)] for k in Y], s = 0.1) \n",
    "plt.xlabel('Non-Spams')\n",
    "plt.ylabel('Spams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70be7ce-5eb0-4bf9-9714-09df2d961c9f",
   "metadata": {},
   "source": [
    "As you can see from the figure, there are quite distinctions for words that appear in spam mails and non-spam mails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d47d47-7b4e-451f-86a2-e088634811d0",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "We will use the sigmoid function to predict our text in which it will return number between 0 and 1. The formula for sigmoid is as follows:\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "\n",
    "Let's refer to z as 'logits'. We can calculate the logits in python by applying **dot product** to vectors $\\theta$ and $x$:\n",
    "\n",
    "$$z = \\theta x$$\n",
    "\n",
    "The shape of $z$ will be `(m, 1)` and applying the sigmoid function $h(z)$ will also return $h$ with shape `(m, 1)`\n",
    "* $m$ is the number of training samples\n",
    "\n",
    "Now let's implement a function `sigmoid` to calculate the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0d424-8c00-4baf-9527-e51876511d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''Return the sigmoid activation of logit\n",
    "    Parameter:\n",
    "        - z: a logit, calculated by multiplying vector theta and vector x\n",
    "    '''\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bf157-51fc-4d6d-b705-454ebe77195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(figsize = (4, 4))\n",
    "\n",
    "zs = np.linspace(-10, 10)\n",
    "\n",
    "ax.plot(zs, [sigmoid(z) for z in zs], color='blue')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('h(z)')\n",
    "plt.title('Sigmoid Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14f984-a531-47fa-9b8f-2e6a0cf1fed7",
   "metadata": {},
   "source": [
    "The graph above visualize the sigmoid function in a cartesian plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653d230-7512-471e-afa2-ca8fbb856f3c",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The cost function will help us calculate the loss of prediction, or how well a machine learning model performs by finding the difference between predicted and actual outputs. Here is the formula for the cost function in a logistic regression:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$: the number of training examples\n",
    "* $y^{(i)}$: the true label of training example 'i'\n",
    "* $h(z^{(i)})$: the sigmoid function or model's prediction for training example 'i'\n",
    "\n",
    "We can perform matrix multiplication to calculate the cost function which will give us this formula:\n",
    "\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* $y$ and $h$ both have shape (m, 1), so we have to transpose the vector $y$ to the left to perform matrix multiplication with dot product\n",
    "* $z$ is calculated by multiplying the feature matrix $x$ with the weight vector 'theta'. $z = x\\theta$\n",
    "* $h$ is calculated with applying sigmoid to each element 'z'. It has a shape of (m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0a867-db10-44d7-b12f-a8aafa689f74",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We will use gradient descent to train our model. The gradient descent will take the derivative of the cost function which is the gradient. With the help of gradient descent, over time we will reduce the cost in which it will make our model more accurate. The learning rate or $\\alpha$ is also crucial and we should pick the right value in order for our gradient descent to not step too much or else we will pass over the global minima of the cost function.\n",
    "\n",
    "Gradient descent will return an improved parameter $\\theta$ that will be helpful in making more accurate prediction.\n",
    "\n",
    "The formula for the gradient descent is as follows:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "\n",
    "$\\nabla_{\\theta_j}J(\\theta)$ is the derivative of the cost function. By implementing this with python, we can use the following formula:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$\n",
    "* We have to transpose $x$ on the left because it's dimension is (m, n+1). Both $h$ and $y$ have shape (m, 1). Then perform matrix operation.\n",
    "\n",
    "Now let's implement function `gradient_descent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd8cbd-115c-4956-add3-075bc7e0fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, iter_count):\n",
    "    '''Perform gradient descent over n number of iterations.\n",
    "    Parameters:\n",
    "        - x: list of features\n",
    "        - y: list of labels\n",
    "        - theta: weight parameters\n",
    "        - alpha: learning rate\n",
    "        - iter_count: number of iterations\n",
    "    '''\n",
    "    m = np.shape(x)[0]\n",
    "    \n",
    "    for i in range(0, iter_count):\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1.0 * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))) / m\n",
    "        # print(f'#{i} Cost: {float(J)}')\n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - alpha / m * np.dot(x.T, h - y)\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f79305-65c2-4b62-a4cd-aacc3b84b5a3",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We already had extracted the features of x and also we have the list of true labels y. Now let's train our model to get optimal parameters that we will use to predict our testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514c16a-04e2-43fe-a508-f38a3f9f18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "J, theta = gradient_descent(X, Y, np.zeros((3, 1)), 1e-9, 800)\n",
    "print(f'Post-training cost: {J}')\n",
    "print(f'Post-training parameters or weights: {[round(t, 8) for t in np.squeeze(theta)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd6a83-4241-4fe7-bbfe-b5553bc652eb",
   "metadata": {},
   "source": [
    "## Predicting mail\n",
    "\n",
    "Now is the time to test our logistic regression and test inputs the model has not seen yet.\n",
    "Let's create a function called `predict_mail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a808a5d-734f-450d-be88-1311874ebc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mail(mail, freqs, theta):\n",
    "    '''Predict a mail and return their classification score.\n",
    "    Parameters:\n",
    "        - mail: a string, particularly a mail\n",
    "        - freqs: a dictionary with a using key (word, label) which contains \n",
    "        counts or how many times a word appeard based on label\n",
    "        - theta: weight parameters\n",
    "    '''\n",
    "    x = extract_features(mail, freqs)\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3625f1-84e7-4614-b3b5-4cf605910db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail = test_x[8]\n",
    "predicted = np.squeeze(predict_mail(mail, freqs, theta))\n",
    "print(f'mail: {mail[:100]}...')\n",
    "print(f'true label: {np.squeeze(test_y)[8]}')\n",
    "print(f'score: {predicted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f88eea-8efd-4894-ab15-dc841778c01e",
   "metadata": {},
   "source": [
    "## Testing logistic regression\n",
    "\n",
    "Let's test our logistic regression and calculate its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbab17d-99c9-4a14-b0a6-5b2405077820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_accuracy(x, y, freqs, theta, predict_mail=predict_mail):\n",
    "    '''Test the logistic regression accuracy. Use test dataset for more accurate forecasting.\n",
    "    Parameters:\n",
    "        - x: list of mails\n",
    "        - y: list of true labels\n",
    "        - freqs: a dictionary with a using key (word, label) which contains \n",
    "        counts or how many times a word appeard based on label\n",
    "        - theta: weight parameters\n",
    "        - predict_mail: prediction function that returns the sigmoid activation result\n",
    "    '''\n",
    "    y_hat = []\n",
    "    \n",
    "    for mail in x:\n",
    "        y_pred = predict_mail(mail, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    \n",
    "    accuracy = np.average([1 if p == True else 0 for p in np.asarray(y_hat) == np.squeeze(test_y)])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494ac39-5824-4b92-bafb-7ac9bc88ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_model_accuracy(test_x, test_y, freqs, theta)\n",
    "print(f'Accuracy Score: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fff358-02f0-4037-9d68-abab76dfad41",
   "metadata": {},
   "source": [
    "A whooping 96%! That is a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f636212-cff2-4354-b2a4-6cdec25dfe64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
